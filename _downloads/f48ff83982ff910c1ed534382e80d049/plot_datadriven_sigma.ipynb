{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Data-driven for dual stepsize choice in sparse recovery\n\nFrom the data, a good dual stepsize can be estimated in the case of sparse\nrecovery.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom celer.datasets import make_correlated_data\nfrom celer.plot_utils import configure_plt\n\nfrom iterreg.sparse import dual_primal\n\nconfigure_plt()\n\n# data for the experiment:\nn_samples = 200\nn_features = 500\n\nX, y, w_true = make_correlated_data(\n    n_samples=n_samples, n_features=n_features,\n    corr=0.2, density=0.1, snr=10, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the L1 case, the Chambolle-Pock algorithm converges to the noisy Basis\nPursuit solution, which has ``min(n_samples, n_features)`` non zero entries.\nThe true coefficients may be much sparser. It is thus important that the\nChambolle-Pock iterates do not get the sparsity of their limit too fast,\nas this would lead to many false positives in the support.\nA remedy is to pick a small enough dual stepsize, $\\sigma$`, so that\nthe dual variable theta grows slowly, and the primal iterates remain sparse\nin the early iterations.\nWith the default stepsizes tau = sigma = 0.99 / ||X||, the iterates become\ndense very fast. If sigma is too small, the iterates stay 0 for too long.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By observing that if theta and w and initialized at 0, the first non zero\nupdate of w is $\\text{shrink}(2 \\tau \\sigma X^\\top y, \\tau)$.\nThe number of non zero coefficients in this vector is the number of indices\n$j$ such that $2 \\sigma |X_j^\\top y| > 1$, hence we pick\n$1 / \\sigma$ as a quantile of $2X^\\top y$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.close('all')\nfig, axarr = plt.subplots(2, 1, sharex=True, constrained_layout=True,\n                          figsize=(7.15, 3))\n\nL = norm(X, ord=2)\nsigma_good = 1. / norm(X.T @ y, ord=np.inf)\nratio_good = 0.99 / (L ** 2 * sigma_good ** 2)\n\nratios = [1, 100, ratio_good, 10000]\nlabels = [r\"$\\sigma=\\tau$\", f\"$\\\\sigma = \\\\tau / {ratios[1]}$\",\n          \"data-driven $\\\\sigma$\",\n          f\"$\\\\sigma = \\\\tau / {ratios[3]}$\", ]\nall_w = dict()\n\nfor ratio, label in zip(ratios, labels):\n    all_w[ratio] = dual_primal(\n        X, y, ret_all=True, max_iter=60, step_ratio=ratio, f_store=1)[-1]\n    f1_scores = [f1_score(w != 0, w_true != 0) for w in all_w[ratio]]\n    supp_size = np.sum(all_w[ratio] != 0, axis=1)\n    axarr[0].plot(f1_scores, label=label)\n    axarr[1].plot(supp_size)\n\naxarr[0].legend(ncol=4, loc='lower right')\naxarr[0].set_ylim(0, 1)\naxarr[0].set_ylabel('F1 score for support')\naxarr[1].set_ylabel(r\"$||x_k||_0$\")\naxarr[1].set_xlabel(r'Iterative regularization iteration')\n\n\nplt.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}