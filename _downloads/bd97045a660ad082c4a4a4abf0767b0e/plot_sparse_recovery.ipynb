{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison with Tikhonov in L1 use for support recovery\n\nThis example compares Tikhonov and iterative regularization to identify the\nsupport of a linear model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom numpy.linalg import norm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom celer.datasets import make_correlated_data\nfrom celer.homotopy import celer_path\n\nfrom iterreg.sparse import dual_primal\nfrom iterreg.utils import datadriven_ratio\n\nn_samples = 500\nn_features = 1_000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function to compute CP, Lasso path and plot metrics:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_varying_sigma(corr, density, snr, max_iter=100, rho=0.99):\n    np.random.seed(0)\n    # true coefficient vector has entries equal to 0 or 1\n    supp = np.random.choice(n_features, size=int(density * n_features),\n                            replace=False)\n    w_true = np.zeros(n_features)\n    w_true[supp] = 1\n    X_, y_, w_true = make_correlated_data(\n        n_samples=int(n_samples * 4 / 3.), n_features=n_features,\n        w_true=w_true,\n        corr=corr, snr=snr, random_state=0)\n\n    X, X_test, y, y_test = train_test_split(X_, y_, test_size=0.25)\n\n    print('Starting computation for this setting')\n\n    ratio = datadriven_ratio(X, y)\n    _, _, _, all_w = dual_primal(\n        X, y, step_ratio=ratio, rho=rho, ret_all=True,\n        max_iter=max_iter,\n        f_store=1)\n\n    fig, axarr = plt.subplots(3, 2, sharey='row', sharex='col',\n                              figsize=(4.2, 4.8), constrained_layout=True)\n\n    fig.suptitle(r\"Correlation=%.1f, $||w^*||_0$= %s, snr=%s\" %\n                 (corr, (w_true != 0).sum(), snr))\n\n    scores = [f1_score(w != 0, w_true != 0) for w in all_w]\n    # supp_size = np.sum(all_w != 0, axis=1)\n    mses = np.array([mean_squared_error(y_test, X_test @ w) for w in all_w])\n\n    axarr[0, 0].plot(scores)\n    axarr[1, 0].plot(norm(all_w - w_true, axis=1) / norm(w_true))\n    axarr[2, 0].plot(mses / np.mean(y_test ** 2))\n\n    axarr[0, 0].set_ylim(0, 1)\n    axarr[0, 0].set_ylabel('F1 score')\n    # axarr[1, 0].set_ylabel(r\"$||w_k||_0$\")\n    axarr[1, 0].set_ylabel(r'$\\Vert w_k - w^*\\Vert / \\Vert w^*\\Vert$')\n    axarr[2, 0].set_ylabel(\"pred MSE left out\")\n    axarr[-1, 0].set_xlabel(\"CP iteration\")\n    axarr[0, 0].set_title('Iterative regularization')\n\n    # last column: Lasso results\n    alphas = norm(X.T @ y, ord=np.inf) / len(y) * np.geomspace(1, 1e-3)\n\n    coefs = celer_path(X, y, 'lasso', alphas=alphas)[1].T\n    axarr[0, 1].semilogx(\n        alphas, [f1_score(coef != 0, w_true != 0) for coef in coefs])\n    # axarr[1, 1].semilogx(\n    #     alphas, [np.sum(coef != 0) for coef in coefs])\n    axarr[1, 1].semilogx(\n        alphas,\n        np.array([norm(coef - w_true) for coef in coefs]) / norm(w_true))\n    axarr[2, 1].semilogx(\n        alphas,\n        np.array([mean_squared_error(y_test, X_test @ coef) for coef in coefs])\n        / np.mean(y_test ** 2))\n\n    axarr[-1, 1].set_xlabel(r'$\\lambda$')\n    axarr[0, 1].set_title(\"Lasso path\")\n\n    for i in range(3):\n        axarr[i, 1].set_xlim(*axarr[i, 1].get_xlim()[::-1])\n\n    plt.show(block=False)\n    return fig\n\n\nplt.close('all')\n# ###############################################################################\n# # Noiseless case where RIP holds (L1 sol = L0 sol)\ndensity = 0.01\ncorr = 0.\nsnr = np.inf\n\nfig = plot_varying_sigma(corr, density, snr, max_iter=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When there is noise in the data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "corr = 0.2\ndensity = 0.1\nsnr = 5\n\nfig = plot_varying_sigma(corr, density, snr, max_iter=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Really difficult case\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "corr = 0.8\ndensity = 0.1\nsnr = 3\n\nfig = plot_varying_sigma(corr, density, snr, max_iter=100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}